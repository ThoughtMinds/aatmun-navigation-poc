from typing_extensions import List, TypedDict
from langchain_core.documents import Document
from app import rag, llm, schema
from langgraph.graph import START, StateGraph
from langchain_core.runnables import chain

vectorstore = rag.get_vectorstore()

NUM_RESULTS = 5


@chain
def retriever(query: str) -> List[Document]:
    docs, scores = zip(
        *vectorstore.similarity_search_with_relevance_scores(
            query,
        )
    )
    for doc, score in zip(docs, scores):
        score = round(score, 2) * 100
        doc.metadata["score"] = score
    return docs


class State(TypedDict):
    """
    Represents the state of the graph.

    Attributes:
        question (str): The question being asked.
        context (List[Document]): A list of documents retrieved from the vector store.
        navigation (schema.Navigation): The navigation information generated by the model.
    """

    question: str
    context: List[Document]
    navigation: schema.Navigation


def retrieve(state: State):
    """
    Retrieves documents from the vector store based on the question in the state.

    Args:
        state (State): The current state of the graph.

    Returns:
        dict: A dictionary containing the retrieved documents in the 'context' key.
    """
    retrieved_docs = retriever.invoke(input=state["question"])
    print(f"Retrieved Docs:\n{retrieved_docs}\n")
    return {"context": retrieved_docs}


def generate(state: State):
    """
    Generates a navigation response based on the retrieved context and the question.

    Args:
        state (State): The current state of the graph.

    Returns:
        dict: A dictionary containing the generated navigation information in the 'navigation' key.
    """
    context = ""
    id_mapping = {}

    for i, doc in enumerate(state["context"], start=1):
        id_mapping[i] = doc.id
        context += (
            f"ID: {i} Score: {doc.metadata['score']} Desciption: {doc.page_content} | "
        )
    print(f"Context:\n{context}\n")
    try:
        first_response = llm.rag_chain.invoke(
            {"query": state["question"], "context": context}
        )
        print(f"Initial Response: {first_response}")
        response = response_validation_with_fallback(
            response=first_response,
            context=state["context"],
            question=state["question"],
            mapping=id_mapping,
        )
        # Regenerate response if not same as vector match and score is >50
        return {"navigation": response}
    except Exception as e:
        print(f"Failed to get Navigation due to: {e}")


def response_validation_with_fallback(
    question: str, context: List[Document], response: schema.Navigation, mapping: dict
) -> schema.Navigation:
    """Validate response and validate. Can result in multiple requests and longer request time

    Args:
        question (str): User query
        context (List[Document]): List of documents to validate response accuracy
        response (schema.Navigation): Original LLM Navigation response

    Returns:
        response: Original or regnerated Navigation
    """
    top_document: Document = context[0]
    id, score = top_document.id, top_document.metadata["score"]

    response.id = mapping.get(response.id, None)
    score_above_threshold = score > 50
    id_mismatch = response.id != id

    print(f"{score_above_threshold=} | {id_mismatch=}")
    if score_above_threshold and response.id != id:
        print(f"[Query (regen)]: {question}")
        response = llm.rag_chain.invoke({"query": question, "context": context})
        print(f"Regenerated Response: {response}")
        response.id = mapping.get(response.id, None)
        if response.id == None:
            response = schema.Navigation(
                id=id, reasoning="Navigation with most semantic similarity"
            )
            print(f"Fallback Response: {response}")
    return response


graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829da226-99bb-48d8-9b3b-cbd7c5e8dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 2.tool-calling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555fd150-ff20-489a-96e4-5e38ed9d33e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A man walked into a library and asked the librarian, \"Do you have any books on Pavlov\\'s dogs and SchrÃ¶dinger\\'s cat?\" The librarian replied, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Tell me a joke\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbff10d-5353-4cde-979a-6d076a34c03f",
   "metadata": {},
   "source": [
    "# Create Simple Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4459e3d2-02df-4a6f-a8f6-fc40ec84edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59221f3f-76fc-4af8-94fe-8e9c8a943f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5cf02a-b879-42a9-aa13-5770b81268c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_list = [search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d13ce53-a097-44a4-b226-069c101d31ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call[\"args\"]\n",
    "            tool_call_id = tool_call[\"id\"]\n",
    "            tool_result = self.tools_by_name[tool_name].invoke(tool_args)\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_name,\n",
    "                    tool_call_id=tool_call_id,\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "def route_tools(\n",
    "    state: State,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50fe76b9-d0ca-4869-80fd-d6bb0d1f433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "base_prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful chatbot. Your task is to assist the user to the best of your ability while making use of available tools. Ensure that your \n",
    "    answers are restricted to the knowledge/tools available to you. If you are not aware of any matter simply inform them \"I don't know\n",
    "    {message_history}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "base_chain = base_prompt_template | llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "181884ad-bd1f-45eb-8b25-f9622b723e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "content_moderation_prompt_template = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are a content moderator. Return True or Falsse based on whether the query is related to safety, safety regulations or not.\n",
    "Query: {query}\n",
    "Response:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def parse(ai_message: AIMessage) -> str:\n",
    "    \"\"\"Parse the AI message.\"\"\"\n",
    "    response = ai_message.content.title()\n",
    "    print(f\"{response=}\")\n",
    "    output_to_bool = response == \"True\"\n",
    "    print(f\"{output_to_bool=}\")\n",
    "    return output_to_bool\n",
    "    \n",
    "moderation_chain = content_moderation_prompt_template | llm | parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85ee3766-8d58-4ea0-aad4-a857ac8b78e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response='I Would Return False For This Query As It Does Not Relate To Safety, Safety Regulations, Or Any Specific Topic That Requires Attention To Safety Protocols.\\n\\nSafety-Related Queries Typically Include Topics Such As:\\n\\n* Safety Guidelines And Regulations (E.G., Osha, Iso)\\n* Emergency Procedures\\n* Hazardous Materials Handling\\n* Workplace Safety\\n\\nNon-Safety Related Queries Might Include:\\n\\n* General Information About A Company Or Organization\\n* A Question About A Specific Product Or Service\\n* A Social Media Post Or Update'\n",
      "output_to_bool=False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(moderation_chain.invoke({\"query\": \"What are some safety measures to take?\"}))\n",
    "# print(moderation_chain.invoke({\"query\": \"Hi\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1aa530-6fae-4c4d-9a2d-425b8378ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.modifier import RemoveMessage\n",
    "\n",
    "def content_moderation_node(state: State):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    print(f\"{last_message=}\") \n",
    "    message_accepted = moderation_chain.invoke({\"query\": last_message.content})\n",
    "    print(f\"{message_accepted=}\") \n",
    "    if not message_accepted:\n",
    "        moderation_response = AIMessage(content=f\"User query: {last_message.content} does not fit our content policy. Warn the user\")\n",
    "        return {\"messages\": state[\"messages\"] + [RemoveMessage(id=last_message.id)] + [moderation_response] }\n",
    "    return {\"messages\": [last_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "752fcd42-e0a7-42c2-b17f-91cca7d93dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": base_chain.invoke(\n",
    "        {\"message_history\": \n",
    "             state[\"messages\"]\n",
    "        }\n",
    "    )\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec341bae-faea-4284-ab36-141d40c68953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"content_moderation_node\", content_moderation_node)\n",
    "tool_node = BasicToolNode(tools=tool_list)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"content_moderation_node\")  # Start at content_moderation_node\n",
    "graph_builder.add_edge(\"content_moderation_node\", \"chatbot\")  # From content_moderation_node to chatbot\n",
    "# graph_builder.add_conditional_edges(\n",
    "#     \"chatbot\",\n",
    "#     route_tools,\n",
    "#     {\"tools\": \"tools\", END: END},\n",
    "# )\n",
    "# graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aa657b-96fb-497a-95d9-5981e7b65b20",
   "metadata": {},
   "source": [
    "### See it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "261c690b-db1e-4088-b777-c47d5ff67334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]:  What are some safety measures to take?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_message=HumanMessage(content='What are some safety measures to take?', additional_kwargs={}, response_metadata={}, id='4bf267cf-a2a0-4a71-942b-b4d01a2d8289')\n",
      "response='I Am Not Able To Provide Information On Safety Measures. Is There Something Else I Can Help You With?'\n",
      "output_to_bool=False\n",
      "message_accepted=False\n",
      "message=('content', '')\n",
      "message=('additional_kwargs', {'tool_calls': [{'id': 'call_wz0v0gsw', 'function': {'arguments': '{\"query\":\"safety measures\"}', 'name': 'duckduckgo_search'}, 'type': 'function', 'index': 0}], 'refusal': None})\n",
      "Message output:\n",
      "{'tool_calls': [{'id': 'call_wz0v0gsw', 'function': {'arguments': '{\"query\":\"safety measures\"}', 'name': 'duckduckgo_search'}, 'type': 'function', 'index': 0}], 'refusal': None}\n",
      "message=('response_metadata', {'token_usage': {'completion_tokens': 36, 'prompt_tokens': 300, 'total_tokens': 336, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'llama3.2:1b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-967', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None})\n",
      "Message output:\n",
      "{'token_usage': {'completion_tokens': 36, 'prompt_tokens': 300, 'total_tokens': 336, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'llama3.2:1b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-967', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}\n",
      "message=('type', 'ai')\n",
      "Message output:\n",
      "ai\n",
      "message=('name', None)\n",
      "message=('id', 'run--a03405f9-e77d-4734-8953-3171832be018-0')\n",
      "Message output:\n",
      "run--a03405f9-e77d-4734-8953-3171832be018-0\n",
      "message=('example', False)\n",
      "message=('tool_calls', [{'name': 'duckduckgo_search', 'args': {'query': 'safety measures'}, 'id': 'call_wz0v0gsw', 'type': 'tool_call'}])\n",
      "Message output:\n",
      "[{'name': 'duckduckgo_search', 'args': {'query': 'safety measures'}, 'id': 'call_wz0v0gsw', 'type': 'tool_call'}]\n",
      "message=('invalid_tool_calls', [])\n",
      "message=('usage_metadata', {'input_tokens': 300, 'output_tokens': 36, 'total_tokens': 336, 'input_token_details': {}, 'output_token_details': {}})\n",
      "Message output:\n",
      "{'input_tokens': 300, 'output_tokens': 36, 'total_tokens': 336, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "from sys import exit\n",
    "from langgraph.types import Command\n",
    "import json\n",
    "\n",
    "\n",
    "user_id = uuid4().hex\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": user_id}}\n",
    "# TODO: Other metadata?\n",
    "\n",
    "\n",
    "try:\n",
    "    user_input = input(\"[User]: \")\n",
    "\n",
    "    for event in graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config=config\n",
    "    ):\n",
    "        \n",
    "        # Check for interrupt\n",
    "        if \"__interrupt__\" in event:\n",
    "            interrupt_obj = event[\"__interrupt__\"][0]\n",
    "            # Usually a list with one Interrupt\n",
    "            prompt = interrupt_obj.value  # The payload sent to human\n",
    "            user_query = prompt[\"query\"]\n",
    "            print(f\"[System]: [User] asks {user_query}\")\n",
    "\n",
    "            # Get human input to resume\n",
    "            human_response = input(\"[Support]: \")\n",
    "\n",
    "            # Resume the graph with human input\n",
    "            resume_events = graph.stream(Command(resume=human_response), config=config)\n",
    "            for resume_event in resume_events:\n",
    "                # Process resumed events (messages, tools, etc.)\n",
    "                print(resume_event)\n",
    "            break  # Exit current event loop to wait for next user input\n",
    "\n",
    "        # Handle chatbot node (tool calls and messages)\n",
    "        if \"chatbot\" in event:\n",
    "            for message in event[\"chatbot\"][\"messages\"]:\n",
    "                # Check for tool calls\n",
    "                if hasattr(message, \"tool_calls\") and message.tool_calls:\n",
    "                    print(\"Tool usage:\")\n",
    "                    for tool_call in message.tool_calls:\n",
    "                        tool_name = tool_call[\"name\"]\n",
    "                        tool_args = tool_call[\"args\"]\n",
    "                        tool_id = tool_call[\"id\"]\n",
    "                        print(f\"{tool_id} | {tool_name} | {tool_args}\")\n",
    "                # Print chatbot message content if no tool calls\n",
    "                else:\n",
    "                    if isinstance(message, tuple):\n",
    "                        # Assuming tuple is (role, content)\n",
    "                        print(f\"{message=}\")\n",
    "                        content = message[1]\n",
    "                    else:\n",
    "                        content = getattr(message, \"content\", None)\n",
    "                    if content:\n",
    "                        print(\"Message output:\")\n",
    "                        print(content)\n",
    "        # Handle tools node (tool execution results)\n",
    "        if \"tools\" in event:\n",
    "            for message in event[\"tools\"][\"messages\"]:\n",
    "                print(\"Tool output:\")\n",
    "                tool_call_id = message.tool_call_id\n",
    "                message_id = message.id\n",
    "                tool_name = message.name\n",
    "                content_preview = message.content[\n",
    "                    :1000\n",
    "                ]  # Limit content to 100 characters\n",
    "                print(\n",
    "                    f\"{tool_call_id} | {message_id} | {tool_name} | {content_preview}\"\n",
    "                )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting...\")\n",
    "    exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72097db9-f15b-4975-b7ae-f877aef97ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
